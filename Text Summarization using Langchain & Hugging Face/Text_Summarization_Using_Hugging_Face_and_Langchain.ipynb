{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text-Summarization\n",
        "\n",
        "### • Stuff: All documents are inserted into a single prompt, which is then passed to an LLM. This is the simplest approach.\n",
        "\n",
        "###  • Map-reduce: Each document is summarized individually (\"map\" step), then those summaries are combined into a final summary (\"reduce\" step). This is a two-stage process that can be more complex but offers more flexibility.\n",
        "\n",
        "### Refine : The refine documents chain constructs a response by looping over the input documents and iteratively updating its answer. For each document, it passes all non-document inputs, the current document, and the latest intermediate answer to an LLM chain to get a new answer."
      ],
      "metadata": {
        "id": "ElymfMdkvTSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Requried libaries"
      ],
      "metadata": {
        "id": "zeujN7BJ3ezC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers einops accelerate langchain bitsandbytes\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "5bGqCCs-vXFc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f66475c-e1bd-482d-d23a-fb11f455a9f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izRMzVkKLYtL",
        "outputId": "f5b5e574-2c40-46f9-e850-d63b4efe7999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.2.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.11.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge\n",
        "!pip install langchainhub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYoDfK373EDF",
        "outputId": "20faa4b1-91e9-4465-d00c-872b5beb39d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Requirement already satisfied: langchainhub in /usr/local/lib/python3.10/dist-packages (0.1.15)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (2.31.0)\n",
            "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (2.31.0.20240406)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "import torch\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader,DirectoryLoader\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from transformers import pipeline\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration"
      ],
      "metadata": {
        "id": "kc14AcArFFIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lamimni-flan-T5-248m pre-trained is used."
      ],
      "metadata": {
        "id": "etnrz_6U3obR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"MBZUAI/LaMini-Flan-T5-248M\"\n",
        "\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(\n",
        "    model_path, torch_dtype=torch.float16, device_map='auto',\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nho2du_yGCt5",
        "outputId": "9532fd5d-2f19-42ca-d354-ce166b333143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the document"
      ],
      "metadata": {
        "id": "Oizm2yZe3zOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(\"The-Hound-of-the-Baskervilles-part1.pdf\")\n",
        "pages = loader.load()"
      ],
      "metadata": {
        "id": "67LdKiZejZ8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### splitting the document into chunk"
      ],
      "metadata": {
        "id": "o6ImzbTM34tP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200,chunk_overlap=50)\n",
        "texts = text_splitter.split_documents(pages)"
      ],
      "metadata": {
        "id": "BL3jqaoCjfrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the pipeline"
      ],
      "metadata": {
        "id": "ngE7BH-03_7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  pipeline = pipeline(\n",
        "    \"summarization\",\n",
        "    model = t5_model,\n",
        "    tokenizer = t5_tokenizer,\n",
        "    max_length=400,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=t5_tokenizer.eos_token_id\n",
        ")"
      ],
      "metadata": {
        "id": "pbb3UUQkViIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### creating the HuggingFace Pipeline"
      ],
      "metadata": {
        "id": "AkctdS-k4D9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0.8})"
      ],
      "metadata": {
        "id": "Hmy-nq-YImt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### creating the llm and prompt template"
      ],
      "metadata": {
        "id": "A_DeBCUD4Lzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain import PromptTemplate,  LLMChain\n",
        "\n",
        "template = \"\"\"\n",
        "              Write a concise summary of the following text delimited by triple backquotes.\n",
        "              Return your response in bullet points which covers the key points of the text.\n",
        "              ```{text}```\n",
        "              BULLET POINT SUMMARY:\n",
        "           \"\"\""
      ],
      "metadata": {
        "id": "IkWMVviiLTI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "Fq3bTFxPMwEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "BgUqjfjaiqCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## stuff documents chain"
      ],
      "metadata": {
        "id": "mYvPyjz54RPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")"
      ],
      "metadata": {
        "id": "gjthpKPwi6uX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stuff_text = stuff_chain.run(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIksF0CJjCVR",
        "outputId": "4f571196-ecb4-42cb-a414-693c63d99a83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (775 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stuff_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "ZNrN42fnz538",
        "outputId": "f695add4-90ff-4233-fd70-cf30559764bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"- Mr. Sherlock Holmes is seated at the breakfast table and picks up a stick from his visitor. - The stick is a fine, thick piece of wood, bulbous-headed, of the sort known as a 'Penang lawyer', with a broad silver band nearly an inch across, engraved upon it with the date '1884.' - Holmes has no sign of his occupation, but he believes that Dr. Mortim, a successful, elder-ly medical man, is well-esteemed since those who know him give him this mark of their appreciation, and the probability is in favor of his being a country practitioner who does a great deal of his visiting on foot.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference_text = \"In the morning, Sherlock Holmes was at the breakfast table, examining a stick left by a visitor. The stick had a silver band engraved with 'To James Mortimer, M.R.C.S., from his friends of the C.C.H., 1884.' Holmes deduced that Dr. Mortimer, an elderly medical man, was likely a country practitioner who walked a lot. The 'C.C.H.' likely referred to a local hunt where Mortimer may have provided surgical assistance. Holmes praised Watson's deductions.\"\n",
        "reference_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "6XtlQeL-1XoF",
        "outputId": "d1f64883-2120-490b-9f4b-476b99ca229f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"In the morning, Sherlock Holmes was at the breakfast table, examining a stick left by a visitor. The stick had a silver band engraved with 'To James Mortimer, M.R.C.S., from his friends of the C.C.H., 1884.' Holmes deduced that Dr. Mortimer, an elderly medical man, was likely a country practitioner who walked a lot. The 'C.C.H.' likely referred to a local hunt where Mortimer may have provided surgical assistance. Holmes praised Watson's deductions.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### evaluting the stuff chain model"
      ],
      "metadata": {
        "id": "DGML_12x4e6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "rouge = Rouge()\n",
        "rouge_scores = rouge.get_scores(stuff_text, reference_text, avg=True)\n",
        "rouge_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYe3_Kwr2iAj",
        "outputId": "668b0284-8b0c-4b92-eca4-b5341cc571f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge-1': {'r': 0.390625,\n",
              "  'p': 0.30864197530864196,\n",
              "  'f': 0.34482758127562424},\n",
              " 'rouge-2': {'r': 0.15853658536585366,\n",
              "  'p': 0.12264150943396226,\n",
              "  'f': 0.13829786742191055},\n",
              " 'rouge-l': {'r': 0.375, 'p': 0.2962962962962963, 'f': 0.3310344778273484}}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sikdQn-t4nvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MapReduceDocumentsChain"
      ],
      "metadata": {
        "id": "G5nE_IiL5ADf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Map\n",
        "map_template = \"\"\"Write a concise summary of the following text delimited by triple backquotes.\n",
        "              Return your response in bullet points which covers the key points of the text.\n",
        "              ```{text}```\n",
        "              BULLET POINT SUMMARY:\"\"\"\n",
        "map_prompt = PromptTemplate.from_template(map_template)\n",
        "map_chain = LLMChain(llm=llm, prompt=map_prompt)"
      ],
      "metadata": {
        "id": "INdlzKij7eXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from langchain import hub\n",
        "\n",
        "map_prompt = hub.pull(\"rlm/map-prompt\")\n",
        "map_chain = LLMChain(llm=llm, prompt=map_prompt)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "fIKAmg4k2_p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce\n",
        "reduce_template = \"\"\"Write a concise summary of the following text delimited by triple backquotes.\n",
        "              Return your response in bullet points which covers the key points of the text.\n",
        "              ```{text}```\n",
        "              BULLET POINT SUMMARY:\"\"\"\n",
        "reduce_prompt = PromptTemplate.from_template(reduce_template)"
      ],
      "metadata": {
        "id": "5aw8Xqol7nLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_prompt = hub.pull(\"rlm/map-prompt\")"
      ],
      "metadata": {
        "id": "f0URjLYv20WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
        "# Run chain\n",
        "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
        "\n",
        "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
        "combine_documents_chain = StuffDocumentsChain(\n",
        "    llm_chain=reduce_chain, document_variable_name=\"docs\"\n",
        ")\n",
        "\n",
        "# Combines and iteratively reduces the mapped documents\n",
        "reduce_documents_chain = ReduceDocumentsChain(\n",
        "    # This is final chain that is called.\n",
        "    combine_documents_chain=combine_documents_chain,\n",
        "    # If documents exceed context for `StuffDocumentsChain`\n",
        "    collapse_documents_chain=combine_documents_chain,\n",
        "    # The maximum number of tokens to group documents into.\n",
        "    token_max=4000,\n",
        ")"
      ],
      "metadata": {
        "id": "JMq5HVq279YO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining documents by mapping a chain over them, then combining results\n",
        "map_reduce_chain = MapReduceDocumentsChain(\n",
        "    # Map chain\n",
        "    llm_chain=map_chain,\n",
        "    # Reduce chain\n",
        "    reduce_documents_chain=reduce_documents_chain,\n",
        "    # The variable name in the llm_chain to put the documents in\n",
        "    document_variable_name=\"docs\",\n",
        "    # Return the results of the map steps in the output\n",
        "    return_intermediate_steps=False,\n",
        ")\n"
      ],
      "metadata": {
        "id": "9xeZXYoM8cXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(\"The-Hound-of-the-Baskervilles-part1.pdf\")\n",
        "pages = loader.load()"
      ],
      "metadata": {
        "id": "163L5N868u3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=50)\n",
        "texts = text_splitter.split_documents(pages)"
      ],
      "metadata": {
        "id": "01VTXxUC9MFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_summarization = map_reduce_chain.run(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kRKTyZX9Se2",
        "outputId": "ad060ef9-23ee-4533-8fee-cbfd5a317d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 400, but your input_length is only 288. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=144)\n",
            "Your max_length is set to 400, but your input_length is only 101. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
            "Your max_length is set to 400, but your input_length is only 292. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=146)\n",
            "Your max_length is set to 400, but your input_length is only 149. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=74)\n",
            "Your max_length is set to 400, but your input_length is only 197. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=98)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(map_summarization)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtPxgO-P9daO",
        "outputId": "846d9fbf-5f2e-4810-8920-92d052cc88d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The main themes in the provided set of documents are Hound of the Baskervilles Chapter 1, Mr. Sherlock Holmes, the presence of a stick with a date engraved on it by a family practitioner, \"eyes in the back of your head,\" \"polished, silver-plated coffee-pot,\" and \"visitor's stick.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "map_summarization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "bBqvTn4ePnVR",
        "outputId": "c8555ceb-6cb0-45e6-af3e-c72f898ecbda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The main themes in the provided set of documents are Hound of the Baskervilles Chapter 1, Mr. Sherlock Holmes, the presence of a stick with a date engraved on it by a family practitioner, \"eyes in the back of your head,\" \"polished, silver-plated coffee-pot,\" and \"visitor\\'s stick.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference_text = \"In the morning, Sherlock Holmes was at the breakfast table, examining a stick left by a visitor. The stick had a silver band engraved with 'To James Mortimer, M.R.C.S., from his friends of the C.C.H., 1884.' Holmes deduced that Dr. Mortimer, an elderly medical man, was likely a country practitioner who walked a lot. The 'C.C.H.' likely referred to a local hunt where Mortimer may have provided surgical assistance. Holmes praised Watson's deductions.\"\n",
        "reference_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "km9ajhPeR2vo",
        "outputId": "d962f5a7-1d1a-4667-d441-44803214c01d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"In the morning, Sherlock Holmes was at the breakfast table, examining a stick left by a visitor. The stick had a silver band engraved with 'To James Mortimer, M.R.C.S., from his friends of the C.C.H., 1884.' Holmes deduced that Dr. Mortimer, an elderly medical man, was likely a country practitioner who walked a lot. The 'C.C.H.' likely referred to a local hunt where Mortimer may have provided surgical assistance. Holmes praised Watson's deductions.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluting the map-reduce chain model"
      ],
      "metadata": {
        "id": "OtJcYvTUSMix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "rouge = Rouge()\n",
        "rouge_scores = rouge.get_scores(map_summarization, reference_text, avg=True)\n",
        "rouge_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcjuyr0ISHev",
        "outputId": "b795ff88-625f-4dcd-f855-5f63bf053260"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge-1': {'r': 0.15625, 'p': 0.2631578947368421, 'f': 0.19607842669742417},\n",
              " 'rouge-2': {'r': 0.036585365853658534,\n",
              "  'p': 0.06521739130434782,\n",
              "  'f': 0.04687499539550827},\n",
              " 'rouge-l': {'r': 0.140625, 'p': 0.23684210526315788, 'f': 0.1764705835601693}}"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Refine"
      ],
      "metadata": {
        "id": "OgWrib15Se9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"Write a concise summary of the following:\n",
        "{text}\n",
        "CONCISE SUMMARY:\"\"\"\n",
        "prompt = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "refine_template = (\n",
        "    \"Your job is to produce a final summary\\n\"\n",
        "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
        "    \"We have the opportunity to refine the existing summary\"\n",
        "    \"(only if needed) with some more context below.\\n\"\n",
        "    \"------------\\n\"\n",
        "    \"{text}\\n\"\n",
        "    \"------------\\n\"\n",
        "    \"Given the new context, refine the original summary in Italian\"\n",
        "    \"If the context isn't useful, return the original summary.\"\n",
        ")\n",
        "refine_prompt = PromptTemplate.from_template(refine_template)\n",
        "chain = load_summarize_chain(\n",
        "    llm=llm,\n",
        "    chain_type=\"refine\",\n",
        "    question_prompt=prompt,\n",
        "    refine_prompt=refine_prompt,\n",
        "    return_intermediate_steps=True,\n",
        "    input_key=\"input_documents\",\n",
        "    output_key=\"output_text\",\n",
        ")\n",
        "result = chain({\"input_documents\": texts}, return_only_outputs=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSBvKCzkSU4L",
        "outputId": "93ef8a9a-c7c3-448e-f9d6-d06c98c4d96b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n",
            "Your max_length is set to 400, but your input_length is only 276. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=138)\n",
            "Your max_length is set to 400, but your input_length is only 250. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=125)\n",
            "Your max_length is set to 400, but your input_length is only 331. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=165)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "refine_summary = result[\"output_text\"]"
      ],
      "metadata": {
        "id": "-GLIsLw7TFhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "refine_summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "GCWO_CHVYKFa",
        "outputId": "061ea28a-3a86-4482-91f6-9f839c1e63d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The Hound of the Baskervilles Chapter 1 is about Mr. Sherlock Holmes, a detective who was seated at the breakfast table. The author picks up a stick from a visitor's hearth-rug, engraved with the date '1884,' and reveals that he was a Penang lawyer. Free eBooks at Planet eBook.comnate become of importance as to miss him and have no notion of his errand, this accidental souvenir becomes of importance. However, we have the opportunity to refine the existing summary(only if needed) with some more context below. 'Really, Watson, you excel yourself,' said Holmes, push-ing back his chair and lighting a cigarette. ‘I am bound to say that in all the accounts which you have been so good as to give of my own small achievements you have habitually underrated your own abilities. It may be that you are not yourself luminous, but you are a conductor of light.'\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference_text = \"Mr. Sherlock Holmes, a detective who was seated at the breakfast table. The author picks up a stick from a visitor's hearth-rug, engraved with the date '1884,' and reveals that he was a Penang lawyer.. The stick had a silver band engraved with 'To James Mortimer, M.R.C.S., from his friends of the C.C.H., 1884.' Holmes deduced that Dr. Mortimer, an elderly medical man, was likely a country practitioner who walked a lot. The 'C.C.H.' likely referred to a local hunt where Mortimer may have provided surgical assistance. Holmes praised Watson's deductions.\"\n",
        "reference_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "Paeqqcx5X1va",
        "outputId": "58adcc53-aa8b-4053-87f3-d7c4c21be156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Mr. Sherlock Holmes, a detective who was seated at the breakfast table. The author picks up a stick from a visitor's hearth-rug, engraved with the date '1884,' and reveals that he was a Penang lawyer.. The stick had a silver band engraved with 'To James Mortimer, M.R.C.S., from his friends of the C.C.H., 1884.' Holmes deduced that Dr. Mortimer, an elderly medical man, was likely a country practitioner who walked a lot. The 'C.C.H.' likely referred to a local hunt where Mortimer may have provided surgical assistance. Holmes praised Watson's deductions.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluting the Refine chain model"
      ],
      "metadata": {
        "id": "nQDzxGCXYAvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "rouge = Rouge()\n",
        "rouge_scores = rouge.get_scores(refine_summary, reference_text, avg=True)\n",
        "rouge_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pb2AeVmaX6S3",
        "outputId": "8743de68-267e-4220-94b4-dfc098362543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge-1': {'r': 0.4864864864864865,\n",
              "  'p': 0.32432432432432434,\n",
              "  'f': 0.38918918438918926},\n",
              " 'rouge-2': {'r': 0.35353535353535354,\n",
              "  'p': 0.2413793103448276,\n",
              "  'f': 0.286885241079347},\n",
              " 'rouge-l': {'r': 0.4864864864864865,\n",
              "  'p': 0.32432432432432434,\n",
              "  'f': 0.38918918438918926}}"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mYakxhsXYU6I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}